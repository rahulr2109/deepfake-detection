{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (64, 64))  # Resize to 64x64 for consistency\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "real_images = load_images_from_folder('path_to_real_images')\n",
    "fake_images = load_images_from_folder('path_to_fake_images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Model |\n",
    "The generator creates fake images from random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, ReLU, Dense, Reshape, Conv2DTranspose, LeakyReLU, Dropout, Flatten, Conv2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256 * 8 * 8, activation=\"relu\", input_dim=100))\n",
    "    model.add(Reshape((8, 8, 256)))\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(ReLU())\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(ReLU())\n",
    "    model.add(Conv2DTranspose(32, kernel_size=4, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(ReLU())\n",
    "    model.add(Conv2D(3, kernel_size=3, padding=\"same\", activation='tanh'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator Model | \n",
    "The discriminator distinguishes between real and fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=(64, 64, 3), padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile GAN\n",
    "\n",
    "Compile the GAN with an optimizer and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def compile_gan(generator, discriminator):\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "    discriminator.trainable = False\n",
    "    gan_input = tf.keras.Input(shape=(100,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = tf.keras.Model(gan_input, gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "    return gan\n",
    "\n",
    "gan = compile_gan(generator, discriminator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimized Training Process\n",
    "Training with Data Augmentation and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, gan, epochs, batch_size):\n",
    "    real_images = np.array(load_images_from_folder('path_to_real_images'))\n",
    "    fake_images = np.array(load_images_from_folder('path_to_fake_images'))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        generated_images = generator.predict(noise)\n",
    "\n",
    "        real_images_batch = real_images[np.random.randint(0, real_images.shape[0], batch_size)]\n",
    "        fake_images_batch = generated_images\n",
    "\n",
    "        combined_images = np.concatenate([real_images_batch, fake_images_batch])\n",
    "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "        labels += 0.05 * np.random.random(labels.shape)\n",
    "\n",
    "        d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        misleading_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, misleading_labels)\n",
    "\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]}] [D accuracy: {d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "train_gan(generator, discriminator, gan, epochs=10000, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_model(generator, discriminator):\n",
    "    noise = np.random.normal(0, 1, (len(real_images), 100))\n",
    "    generated_images = generator.predict(noise)\n",
    "\n",
    "    real_labels = np.ones((len(real_images), 1))\n",
    "    fake_labels = np.zeros((len(fake_images), 1))\n",
    "\n",
    "    real_predictions = discriminator.predict(np.array(real_images))\n",
    "    fake_predictions = discriminator.predict(generated_images)\n",
    "\n",
    "    y_true = np.concatenate([real_labels, fake_labels])\n",
    "    y_pred = np.concatenate([real_predictions, fake_predictions])\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred > 0.5)\n",
    "    precision = precision_score(y_true, y_pred > 0.5)\n",
    "    recall = recall_score(y_true, y_pred > 0.5)\n",
    "    f1 = f1_score(y_true, y_pred > 0.5)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}, ROC-AUC: {roc_auc}\")\n",
    "\n",
    "evaluate_model(generator, discriminator)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
